{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234cdcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanan\\anaconda3\\python.exe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aanan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aanan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# !pip install ftfy\n",
    "import ftfy\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import exp\n",
    "from numpy import sign\n",
    "import sys\n",
    "print(sys.executable)\n",
    "from PIL import Image # getting images in notebook\n",
    "# !pip install gensim\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import  classification_report, confusion_matrix, accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# !pip install tensorflow\n",
    "\n",
    "# !pip install tensorflow_hub\n",
    "\n",
    "# !pip install bert-for-tf2\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51a5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad135d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r'C:\\Users\\aanan\\Documents\\Major-1\\Datasets\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236cbc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>train_pid_4559</td>\n",
       "      <td>Comparing myself to others... : I’ve found tha...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>train_pid_1016</td>\n",
       "      <td>I sometimes feel like it’s not depression : Ke...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6101</th>\n",
       "      <td>train_pid_6102</td>\n",
       "      <td>WhatsApp support group for the depressed : [re...</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  \\\n",
       "4558  train_pid_4559  Comparing myself to others... : I’ve found tha...   \n",
       "1015  train_pid_1016  I sometimes feel like it’s not depression : Ke...   \n",
       "6101  train_pid_6102  WhatsApp support group for the depressed : [re...   \n",
       "\n",
       "               Label  \n",
       "4558        moderate  \n",
       "1015        moderate  \n",
       "6101  not depression  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf8f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(r'C:\\Users\\aanan\\Documents\\Major-1\\Datasets\\dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c7e06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 8891 rows and 3 columns.\n",
      "Test set has 4496 rows and 3 columns.\n",
      "\n",
      "Index(['PID', 'Text_data', 'Label'], dtype='object')\n",
      "Index(['PID', 'Text data', 'Label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set has {} rows and {} columns.\".format(train.shape[0], train.shape[1]))\n",
    "print(\"Test set has {} rows and {} columns.\".format(test.shape[0], test.shape[1]))\n",
    "\n",
    "print()\n",
    "print(train.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "775a8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment = {'moderate': 1,'not depression': 0,'severe':2}\n",
    "train.Label = [Sentiment[item] for item in train.Label]\n",
    "test.Label= [Sentiment[item] for item in test.Label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fba5cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count NaN:\n",
      "PID          0\n",
      "Text_data    0\n",
      "Label        0\n",
      "dtype: int64 \n",
      "\n",
      "Percentage NaN:\n",
      "PID          0.0\n",
      "Text_data    0.0\n",
      "Label        0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Count NaN:')\n",
    "print(train.isnull().sum(), '\\n')\n",
    "print('Percentage NaN:')\n",
    "print(train.isnull().sum()/ len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f451442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count NaN:\n",
      "PID          0\n",
      "Text data    0\n",
      "Label        0\n",
      "dtype: int64 \n",
      "\n",
      "Percentage NaN:\n",
      "PID          0.0\n",
      "Text data    0.0\n",
      "Label        0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Count NaN:')\n",
    "print(test.isnull().sum(), '\\n')\n",
    "print('Percentage NaN:')\n",
    "print(test.isnull().sum()/ len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18246cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# NLTK Tweet Tokenizer for now\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# clean up text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Copied from other notebooks\n",
    "    \"\"\"\n",
    "    # expand acronyms\n",
    "    \n",
    "    # special characters\n",
    "    text = re.sub(r\"\\x89Û_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n",
    "    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n",
    "    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n",
    "    text = re.sub(r\"\\x89Û÷\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Ûª\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n",
    "    text = re.sub(r\"å_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n",
    "    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n",
    "    text = re.sub(r\"åÊ\", \"\", text)\n",
    "    text = re.sub(r\"åÈ\", \"\", text)\n",
    "    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n",
    "    text = re.sub(r\"Ì©\", \"e\", text)\n",
    "    text = re.sub(r\"å¨\", \"\", text)\n",
    "    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n",
    "    text = re.sub(r\"åÇ\", \"\", text)\n",
    "    text = re.sub(r\"å£3million\", \"3 million\", text)\n",
    "    text = re.sub(r\"åÀ\", \"\", text)\n",
    "    \n",
    "    # emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Our Stuff\n",
    "    \"\"\"\n",
    "    # remove numbers\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    \n",
    "    # remove punctuation and special chars (keep '!')\n",
    "    for p in string.punctuation.replace('!', ''):\n",
    "        text = text.replace(p, '')\n",
    "        \n",
    "    # remove urls\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # tokenize\n",
    "    text = tknzr.tokenize(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = [w.lower() for w in text if not w in stop_words]\n",
    "    corpus.append(text)\n",
    "    \n",
    "    # join back\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e4c91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.3 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8408</th>\n",
       "      <td>train_pid_8409</td>\n",
       "      <td>what so i ’ grown parents good pair they fough...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>train_pid_1206</td>\n",
       "      <td>forever lost f i ’ severe depression since i g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7815</th>\n",
       "      <td>train_pid_7816</td>\n",
       "      <td>fuck everything week particular i hate week ev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>train_pid_424</td>\n",
       "      <td>happy new year fuck bettexaxaxaxaxaxa even hap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>train_pid_4869</td>\n",
       "      <td>depression much phone time around months ago i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8191</th>\n",
       "      <td>train_pid_8192</td>\n",
       "      <td>im gonna die removed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>train_pid_2726</td>\n",
       "      <td>lost forever i wish i knew felt loved somethin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3159</th>\n",
       "      <td>train_pid_3160</td>\n",
       "      <td>i cried front parents yesterday now feel weak ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>train_pid_2595</td>\n",
       "      <td>cat died new years day we really close he alwa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>train_pid_482</td>\n",
       "      <td>spending nye alone car this year sucked say le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  Label\n",
       "8408  train_pid_8409  what so i ’ grown parents good pair they fough...      2\n",
       "1205  train_pid_1206  forever lost f i ’ severe depression since i g...      1\n",
       "7815  train_pid_7816  fuck everything week particular i hate week ev...      0\n",
       "423    train_pid_424  happy new year fuck bettexaxaxaxaxaxa even hap...      1\n",
       "4868  train_pid_4869  depression much phone time around months ago i...      1\n",
       "8191  train_pid_8192                               im gonna die removed      2\n",
       "2725  train_pid_2726  lost forever i wish i knew felt loved somethin...      1\n",
       "3159  train_pid_3160  i cried front parents yesterday now feel weak ...      1\n",
       "2594  train_pid_2595  cat died new years day we really close he alwa...      1\n",
       "481    train_pid_482  spending nye alone car this year sucked say le...      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train['Text_data'] = train['Text_data'].apply(lambda s: clean_text(s))\n",
    "test['Text data'] = test['Text data'].apply(lambda s: clean_text(s))\n",
    "\n",
    "# see some cleaned data\n",
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ead7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train['Text_data'].to_numpy()\n",
    "word_freq = {}\n",
    "\n",
    "for text in texts:\n",
    "    for word in text.split():\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7e95bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13266 unique tokens.\n",
      "Shape of data tensor: (8891, 40)\n",
      "Shape of label tensor: (8891,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "print('Found %s unique tokens.' % (num_words - 1))\n",
    "\n",
    "# pad \n",
    "data = pad_sequences(\n",
    "    sequences, \n",
    "    maxlen=MAX_SEQUENCE_LENGTH,\n",
    "    padding='post', \n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "labels = train['Label'].to_numpy()\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a8c8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63ed2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(train['Text_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f865140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88f8aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, Y_train, Y_cv = train_test_split(train[\"Text_data\"], train[\"Label\"], test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42b05a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6223, 11938)\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_train = X_train.toarray()\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6342f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2668, 11938)\n"
     ]
    }
   ],
   "source": [
    "X_cv = vectorizer.transform(X_cv)\n",
    "X_cv = X_cv.toarray()\n",
    "print(X_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5786a72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4496, 11938)\n"
     ]
    }
   ],
   "source": [
    "X_test = vectorizer.transform(test[\"Text data\"])\n",
    "X_test = X_test.toarray()\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "533c85df",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression() \n",
    "classifier = classifier.fit( X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7847e89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8849325337331334\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(X_cv) \n",
    "print(\"Accuracy: \", accuracy_score(Y_cv, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76a81ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.75      0.80       598\n",
      "           1       0.90      0.95      0.93      1797\n",
      "           2       0.83      0.73      0.78       273\n",
      "\n",
      "    accuracy                           0.88      2668\n",
      "   macro avg       0.86      0.81      0.83      2668\n",
      "weighted avg       0.88      0.88      0.88      2668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_cv,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f45c6",
   "metadata": {},
   "source": [
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(train['Text_data'].values,train['Label'].values,test_size=0.2,random_state=123,stratify=train['Label'].values)\n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "\n",
    "tfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "tfidf_test_vectors = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d98dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "\n",
    "classifier.fit(tfidf_train_vectors,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af211007",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(tfidf_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86bd54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fef8df3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
