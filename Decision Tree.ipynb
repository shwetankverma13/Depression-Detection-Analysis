{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6c1740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHWETANK VERMA\\Anaconda3\\latest\\python.exe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SHWETANK\n",
      "[nltk_data]     VERMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\SHWETANK\n",
      "[nltk_data]     VERMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# !pip install ftfy\n",
    "import ftfy\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import exp\n",
    "from numpy import sign\n",
    "import sys\n",
    "print(sys.executable)\n",
    "from PIL import Image # getting images in notebook\n",
    "# !pip install gensim\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import  classification_report, confusion_matrix, accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# !pip install tensorflow\n",
    "\n",
    "# !pip install tensorflow_hub\n",
    "\n",
    "# !pip install bert-for-tf2\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77992363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras \n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "414ac5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r'C:\\Users\\SHWETANK VERMA\\Documents\\Mlstuff\\Major-1\\Datasets\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2966d83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7934</th>\n",
       "      <td>train_pid_7935</td>\n",
       "      <td>I have panic attacks a lot, like once or twice...</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7123</th>\n",
       "      <td>train_pid_7124</td>\n",
       "      <td>Awake for 15 hours before I talked to another ...</td>\n",
       "      <td>not depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5102</th>\n",
       "      <td>train_pid_5103</td>\n",
       "      <td>New Year, Same Loneliness : Not like I hoped t...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  \\\n",
       "7934  train_pid_7935  I have panic attacks a lot, like once or twice...   \n",
       "7123  train_pid_7124  Awake for 15 hours before I talked to another ...   \n",
       "5102  train_pid_5103  New Year, Same Loneliness : Not like I hoped t...   \n",
       "\n",
       "               Label  \n",
       "7934  not depression  \n",
       "7123  not depression  \n",
       "5102        moderate  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee79241",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(r'C:\\Users\\SHWETANK VERMA\\Documents\\Mlstuff\\Major-1\\Datasets\\dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e46be5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 8891 rows and 3 columns.\n",
      "Test set has 4496 rows and 3 columns.\n",
      "\n",
      "Index(['PID', 'Text_data', 'Label'], dtype='object')\n",
      "Index(['PID', 'Text data', 'Label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set has {} rows and {} columns.\".format(train.shape[0], train.shape[1]))\n",
    "print(\"Test set has {} rows and {} columns.\".format(test.shape[0], test.shape[1]))\n",
    "\n",
    "print()\n",
    "print(train.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01be3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment = {'moderate': 1,'not depression': 0,'severe':2}\n",
    "train.Label = [Sentiment[item] for item in train.Label]\n",
    "test.Label= [Sentiment[item] for item in test.Label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67dd6395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5352</th>\n",
       "      <td>train_pid_5353</td>\n",
       "      <td>Can I take a moment just to rant? : So, my gf’...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6219</th>\n",
       "      <td>train_pid_6220</td>\n",
       "      <td>I want to help my very depressed friend but id...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>train_pid_4398</td>\n",
       "      <td>part of me won’t leave : i try to better mysel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  Label\n",
       "5352  train_pid_5353  Can I take a moment just to rant? : So, my gf’...      1\n",
       "6219  train_pid_6220  I want to help my very depressed friend but id...      0\n",
       "4397  train_pid_4398  part of me won’t leave : i try to better mysel...      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9804349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>dev_pid_1337</td>\n",
       "      <td>I’ve been in quarantine in my head for 2 years...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>dev_pid_2034</td>\n",
       "      <td>How bad is bad enough? : Hi :) \\nThis is my fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>dev_pid_2760</td>\n",
       "      <td>Had a huge fight with my parents- blamed them ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PID                                          Text data  Label\n",
       "1336  dev_pid_1337  I’ve been in quarantine in my head for 2 years...      1\n",
       "2033  dev_pid_2034  How bad is bad enough? : Hi :) \\nThis is my fi...      1\n",
       "2759  dev_pid_2760  Had a huge fight with my parents- blamed them ...      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "633f9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# NLTK Tweet Tokenizer for now\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# clean up text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Copied from other notebooks\n",
    "    \"\"\"\n",
    "    # expand acronyms\n",
    "    \n",
    "    # special characters\n",
    "    text = re.sub(r\"\\x89Û_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n",
    "    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n",
    "    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n",
    "    text = re.sub(r\"\\x89Û÷\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Ûª\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n",
    "    text = re.sub(r\"å_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n",
    "    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n",
    "    text = re.sub(r\"åÊ\", \"\", text)\n",
    "    text = re.sub(r\"åÈ\", \"\", text)\n",
    "    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n",
    "    text = re.sub(r\"Ì©\", \"e\", text)\n",
    "    text = re.sub(r\"å¨\", \"\", text)\n",
    "    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n",
    "    text = re.sub(r\"åÇ\", \"\", text)\n",
    "    text = re.sub(r\"å£3million\", \"3 million\", text)\n",
    "    text = re.sub(r\"åÀ\", \"\", text)\n",
    "    \n",
    "    # emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Our Stuff\n",
    "    \"\"\"\n",
    "    # remove numbers\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    \n",
    "    # remove punctuation and special chars (keep '!')\n",
    "    for p in string.punctuation.replace('!', ''):\n",
    "        text = text.replace(p, '')\n",
    "        \n",
    "    # remove urls\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # tokenize\n",
    "    text = tknzr.tokenize(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = [w.lower() for w in text if not w in stop_words]\n",
    "    corpus.append(text)\n",
    "    \n",
    "    # join back\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "436fb920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>train_pid_1395</td>\n",
       "      <td>i want stop removed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>train_pid_703</td>\n",
       "      <td>alone nye another holiday im tired alone time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8091</th>\n",
       "      <td>train_pid_8092</td>\n",
       "      <td>i ’ want live life sick tired i chronic painfa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>train_pid_2759</td>\n",
       "      <td>feeling alone holiday i spend next contact fam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4586</th>\n",
       "      <td>train_pid_4587</td>\n",
       "      <td>i tried cut couldnt i need someone talk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>train_pid_7009</td>\n",
       "      <td>i ’ proud alive making this year tough i ’ sur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6510</th>\n",
       "      <td>train_pid_6511</td>\n",
       "      <td>imaa fishhhh removed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>train_pid_3272</td>\n",
       "      <td>those learned feel happiness life pointless wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>train_pid_4222</td>\n",
       "      <td>everyday physical pain i chronic pain conditio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6459</th>\n",
       "      <td>train_pid_6460</td>\n",
       "      <td>happy new year i started sending friends text ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  Label\n",
       "1394  train_pid_1395                                i want stop removed      1\n",
       "702    train_pid_703  alone nye another holiday im tired alone time ...      1\n",
       "8091  train_pid_8092  i ’ want live life sick tired i chronic painfa...      2\n",
       "2758  train_pid_2759  feeling alone holiday i spend next contact fam...      1\n",
       "4586  train_pid_4587            i tried cut couldnt i need someone talk      1\n",
       "7008  train_pid_7009  i ’ proud alive making this year tough i ’ sur...      0\n",
       "6510  train_pid_6511                               imaa fishhhh removed      0\n",
       "3271  train_pid_3272  those learned feel happiness life pointless wi...      1\n",
       "4221  train_pid_4222  everyday physical pain i chronic pain conditio...      1\n",
       "6459  train_pid_6460  happy new year i started sending friends text ...      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train['Text_data'] = train['Text_data'].apply(lambda s: clean_text(s))\n",
    "test['Text data'] = test['Text data'].apply(lambda s: clean_text(s))\n",
    "\n",
    "# see some cleaned data\n",
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffee21bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy-download in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy-download) (3.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (63.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (2.28.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (1.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (0.7.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (0.4.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (3.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (8.0.17)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (1.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (2.4.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-download) (4.64.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy-download) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy-download) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-download) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-download) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-download) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-download) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<4.0.0,>=3.0.0->spacy-download) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-download) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy-download) (2.0.1)\n",
      "Requirement already satisfied: allennlp in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (2.10.1)\n",
      "Requirement already satisfied: pytest>=6.2.5 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (7.1.2)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (2.28.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (0.3.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.16 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (0.10.1)\n",
      "Requirement already satisfied: spacy<3.4,>=2.1.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (3.3.2)\n",
      "Requirement already satisfied: traitlets>5.1.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (5.9.0)\n",
      "Requirement already satisfied: fairscale==0.4.6 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (0.4.6)\n",
      "Requirement already satisfied: termcolor==1.1.0 in c:\\users\\shwetank verma\\appdata\\roaming\\python\\python39\\site-packages (from allennlp) (1.1.0)\n",
      "Requirement already satisfied: transformers<4.21,>=4.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (4.20.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (1.0.2)\n",
      "Requirement already satisfied: typer>=0.4.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (0.4.2)\n",
      "Requirement already satisfied: more-itertools>=8.12.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (9.1.0)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (3.19.6)\n",
      "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (0.12.21)\n",
      "Requirement already satisfied: base58>=2.1.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (2.1.1)\n",
      "Requirement already satisfied: nltk>=3.6.5 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (3.7)\n",
      "Requirement already satisfied: scipy>=1.7.3 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (1.9.1)\n",
      "Requirement already satisfied: sentencepiece>=0.1.96 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (0.1.97)\n",
      "Requirement already satisfied: tqdm>=4.62 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (4.64.1)\n",
      "Requirement already satisfied: lmdb>=1.2.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (1.4.1)\n",
      "Requirement already satisfied: cached-path<1.2.0,>=1.1.3 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (1.1.6)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (0.0.53)\n",
      "Requirement already satisfied: tensorboardX>=1.2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (2.6)\n",
      "Requirement already satisfied: torchvision<0.14.0,>=0.8.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (0.13.1)\n",
      "Requirement already satisfied: h5py>=3.6.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (3.7.0)\n",
      "Requirement already satisfied: torch<1.13.0,>=1.10.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (1.12.1)\n",
      "Requirement already satisfied: filelock<3.8,>=3.3 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.21.4 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from allennlp) (1.21.5)\n",
      "Requirement already satisfied: rich<13.0,>=12.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (12.6.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (1.24.28)\n",
      "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from huggingface-hub>=0.0.16->allennlp) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from huggingface-hub>=0.0.16->allennlp) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from huggingface-hub>=0.0.16->allennlp) (4.3.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from nltk>=3.6.5->allennlp) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from nltk>=3.6.5->allennlp) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from nltk>=3.6.5->allennlp) (1.1.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from pytest>=6.2.5->allennlp) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from pytest>=6.2.5->allennlp) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from pytest>=6.2.5->allennlp) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from pytest>=6.2.5->allennlp) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from pytest>=6.2.5->allennlp) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from pytest>=6.2.5->allennlp) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from requests>=2.28->allennlp) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from requests>=2.28->allennlp) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from requests>=2.28->allennlp) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from requests>=2.28->allennlp) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from scikit-learn>=1.0.1->allennlp) (2.2.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (63.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (5.2.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (8.0.17)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (1.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.11.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (9.2.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from transformers<4.21,>=4.1->allennlp) (0.12.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.4.0)\n",
      "Requirement already satisfied: pathtools in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.1.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.9.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.1.31)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.0.11)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.20.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.3.2)\n",
      "Requirement already satisfied: six>=1.13.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.16.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.27.28)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.6.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (4.0.10)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.14.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.4.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.0.16->allennlp) (3.0.9)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.11.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\shwetank verma\\appdata\\roaming\\python\\python39\\site-packages (from botocore<1.28.0,>=1.27.28->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (5.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.59.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.2.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\shwetank verma\\anaconda3\\latest\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy-download\n",
    "!pip install allennlp\n",
    "# python -m spacy download en_core_web_sm\n",
    "# import spacy\n",
    "# print(spacy.__file__)\n",
    "# import sys\n",
    "# print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6626a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install allennlp\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    # Remove stop words and lemmatize the tokens\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "    # Join the tokens back into a string\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5e40696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the training and testing text data\n",
    "train[\"Text_data\"] = train[\"Text_data\"].apply(preprocess_text)\n",
    "test[\"Text data\"] = test[\"Text data\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f144d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wait mind breakdown new year feeling anymore know little bit worried depressed day time year try breakdown start mere day later break cry entire year december ok month wait weird way act feel feel bit normal'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Text_data\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fb09013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractF(df):\n",
    "    verbs = []\n",
    "    args = []\n",
    "\n",
    "    for text in df[\"Text_data\"]:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                verbs.append(token.text)\n",
    "                for child in token.children:\n",
    "                    if child.pos_ != \"PUNCT\":\n",
    "                        args.append(child.text)\n",
    "\n",
    "    # Create a vocabulary of unique verbs and arguments\n",
    "    vocab = list(set(verbs + args))\n",
    "    \n",
    "    for verb in verbs:\n",
    "        df[verb] = df[\"Text_data\"].apply(lambda x: 1 if verb in x else 0)\n",
    "\n",
    "    for arg in args:\n",
    "        df[arg] = df[\"Text_data\"].apply(lambda x: 1 if arg in x else 0)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f6857c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractF2(df):\n",
    "    verbs = []\n",
    "    args = []\n",
    "\n",
    "    for text in df[\"Text data\"]:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                verbs.append(token.text)\n",
    "                for child in token.children:\n",
    "                    if child.pos_ != \"PUNCT\":\n",
    "                        args.append(child.text)\n",
    "\n",
    "    # Create a vocabulary of unique verbs and arguments\n",
    "    vocab = list(set(verbs + args))\n",
    "    \n",
    "    for verb in verbs:\n",
    "        df[verb] = df[\"Text data\"].apply(lambda x: 1 if verb in x else 0)\n",
    "\n",
    "    for arg in args:\n",
    "        df[arg] = df[\"Text data\"].apply(lambda x: 1 if arg in x else 0)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec200014",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab1 = extractF(train)\n",
    "vocab2 = extractF2(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1743c444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['skater',\n",
       " 'blow',\n",
       " 'testosterone',\n",
       " 'teleport',\n",
       " 'income',\n",
       " 'multitude',\n",
       " 'connect',\n",
       " 'dissipate',\n",
       " 'doing',\n",
       " 'depress',\n",
       " 'grow',\n",
       " 'cureall',\n",
       " 'waitlist',\n",
       " 'agonisingly',\n",
       " 'crown',\n",
       " 'perception',\n",
       " 'legitimately',\n",
       " 'childhood',\n",
       " 'skull',\n",
       " 'becuse',\n",
       " 'recognize',\n",
       " 'having',\n",
       " 'roll',\n",
       " 'enrol',\n",
       " 'medicated',\n",
       " 'seeked',\n",
       " 'fruitlessly',\n",
       " 'remorse',\n",
       " 'praise',\n",
       " 'reconnect',\n",
       " 'unbelievable',\n",
       " 'awakening',\n",
       " 'welland',\n",
       " 'sunway',\n",
       " 'render',\n",
       " 'increasingly',\n",
       " 'pizza',\n",
       " 'happend',\n",
       " 'selfisolate',\n",
       " 'workmate',\n",
       " 'computing',\n",
       " 'teaching',\n",
       " 'harmless',\n",
       " 'mitigated',\n",
       " 'whouldnt',\n",
       " 'diaper',\n",
       " 'foreverfuck',\n",
       " 'msg',\n",
       " 'stay',\n",
       " 'eyelash',\n",
       " 'reamber',\n",
       " 'accident',\n",
       " 'plllease',\n",
       " 'fro',\n",
       " 'assault',\n",
       " 'lining',\n",
       " 'happy',\n",
       " 'romantically',\n",
       " 'everlasting',\n",
       " 'probaly',\n",
       " 'imagery',\n",
       " 'will',\n",
       " 'activate',\n",
       " 'couple',\n",
       " 'midst',\n",
       " 'preinstalle',\n",
       " 'questioning',\n",
       " 'clause',\n",
       " 'breakup',\n",
       " 'swiftly',\n",
       " 'curl',\n",
       " 'unsurprisingly',\n",
       " 'cafe',\n",
       " 'bryant',\n",
       " 'amid',\n",
       " 'automationrobot',\n",
       " 'energy',\n",
       " 'hopes',\n",
       " 'cause',\n",
       " 'lessprivileged',\n",
       " 'bla',\n",
       " 'becaue',\n",
       " 'consciously',\n",
       " 'doll',\n",
       " 'apa',\n",
       " 'skip',\n",
       " 'squidward',\n",
       " 'uphousework',\n",
       " 'outright',\n",
       " 'simply',\n",
       " 'berate',\n",
       " 'hairline',\n",
       " 'vanilla',\n",
       " 'salary',\n",
       " 'deadbeat',\n",
       " 'held',\n",
       " 'diferent',\n",
       " 'perform',\n",
       " 'welbutrin',\n",
       " 'punishing',\n",
       " 'panicky',\n",
       " 'feedback',\n",
       " 'poison',\n",
       " 'bear',\n",
       " 'embrace',\n",
       " 'concise',\n",
       " 'anonimus',\n",
       " 'sternly',\n",
       " 'skyrocket',\n",
       " 'airport',\n",
       " 'clam',\n",
       " 'tiredness',\n",
       " 'esay',\n",
       " 'hae',\n",
       " 'naturally',\n",
       " 'yearsit',\n",
       " 'endocrinologist',\n",
       " 'isolated',\n",
       " 'coddle',\n",
       " 'edge',\n",
       " 'workforce',\n",
       " 'relapse',\n",
       " 'cubao',\n",
       " 'tran',\n",
       " 'cafesrestaurantssport',\n",
       " 'nigg',\n",
       " 'steak',\n",
       " 'disassociate',\n",
       " 'deserving',\n",
       " 'nada',\n",
       " 'option',\n",
       " 'obnoxious',\n",
       " 'enjoyable',\n",
       " 'marker',\n",
       " 'bonus',\n",
       " 'video',\n",
       " 'calmer',\n",
       " 'cycle',\n",
       " 'rapist',\n",
       " 'themself',\n",
       " 'vibration',\n",
       " 'logically',\n",
       " 'tolerate',\n",
       " 'wrench',\n",
       " 'injustice',\n",
       " 'tissue',\n",
       " 'npc',\n",
       " 'girth',\n",
       " 'swimming',\n",
       " 'ambulance',\n",
       " 'whet',\n",
       " 'rail',\n",
       " 'uglier',\n",
       " 'autism',\n",
       " 'te',\n",
       " 'exponentially',\n",
       " 'mebecause',\n",
       " 'slat',\n",
       " 'rit',\n",
       " 'punching',\n",
       " 'goodness',\n",
       " 'helpme',\n",
       " 'laughte',\n",
       " 'likely',\n",
       " 'capitulate',\n",
       " 'foot',\n",
       " 'vectore',\n",
       " 'oflye',\n",
       " 'dissapointe',\n",
       " 'therefor',\n",
       " 'manifestation',\n",
       " 'goddamne',\n",
       " 'quetion',\n",
       " 'incorrectly',\n",
       " 'digitize',\n",
       " 'minority',\n",
       " 'fetish',\n",
       " 'surely',\n",
       " 'anchored',\n",
       " 'misinterpret',\n",
       " 'unsatisfyingly',\n",
       " 'offmych',\n",
       " 'mgs',\n",
       " 'selfharme',\n",
       " 'reference',\n",
       " 'sorrowfully',\n",
       " 'ward',\n",
       " 'reset',\n",
       " 'wanting',\n",
       " 'offend',\n",
       " 'proud',\n",
       " 'subtle',\n",
       " 'stepdad',\n",
       " 'exist',\n",
       " 'avout',\n",
       " 'bean',\n",
       " 'zoning',\n",
       " 'rumour',\n",
       " 'dynamic',\n",
       " 'cp',\n",
       " 'powerful',\n",
       " 'sided',\n",
       " 'workedi',\n",
       " 'lmfao',\n",
       " 'overpower',\n",
       " 'input',\n",
       " 'compilation',\n",
       " 'principal',\n",
       " 'convinient',\n",
       " 'shallowness',\n",
       " 'mh',\n",
       " 'remembe',\n",
       " 'introvert',\n",
       " 'gnawing',\n",
       " 'crochet',\n",
       " 'shortcut',\n",
       " 'maintenance',\n",
       " 'whiskey',\n",
       " 'lol',\n",
       " 'dimly',\n",
       " 'misunderstood',\n",
       " 'repair',\n",
       " 'didlost',\n",
       " 'toy',\n",
       " 'dice',\n",
       " 'cliché',\n",
       " 'intense',\n",
       " 'unusual',\n",
       " 'applying',\n",
       " 'entertaining',\n",
       " 'shepard',\n",
       " 'er',\n",
       " 'walk',\n",
       " 'request',\n",
       " 'suppressant',\n",
       " 'spam',\n",
       " 'spanking',\n",
       " 'parenting',\n",
       " 'console',\n",
       " 'joyfulness',\n",
       " 'bc',\n",
       " 'lift',\n",
       " 'counseling',\n",
       " 'slump',\n",
       " 'год',\n",
       " 'nicotine',\n",
       " 'asylum',\n",
       " 'swell',\n",
       " 'havnt',\n",
       " 'amnot',\n",
       " 'imim',\n",
       " 'hest',\n",
       " 'october',\n",
       " 'pave',\n",
       " 'condescend',\n",
       " 'memorable',\n",
       " 'depersonalizationderealization',\n",
       " 'judge',\n",
       " 'smacking',\n",
       " 'opponent',\n",
       " 'deranke',\n",
       " 'smooth',\n",
       " 'placebo',\n",
       " 'ocupied',\n",
       " 'delusionaldisconnecte',\n",
       " 'incidentally',\n",
       " 'aspect',\n",
       " 'roundabout',\n",
       " 'doen',\n",
       " 'finasteride',\n",
       " 'enslave',\n",
       " 'stability',\n",
       " 'xray',\n",
       " 'equivocate',\n",
       " 'gt',\n",
       " 'crumble',\n",
       " 'understatement',\n",
       " 'suspend',\n",
       " 'bored',\n",
       " 'instantaneously',\n",
       " 'chelsea',\n",
       " 'loafer',\n",
       " 'flesh',\n",
       " 'mood',\n",
       " 'crushclose',\n",
       " 'compass',\n",
       " 'melt',\n",
       " 'argue',\n",
       " 'mw',\n",
       " 'engineeee',\n",
       " 'implement',\n",
       " 'strong',\n",
       " 'separated',\n",
       " 'pneumonia',\n",
       " 'mail',\n",
       " 'adore',\n",
       " 'crypto',\n",
       " 'um',\n",
       " 'accurately',\n",
       " 'adida',\n",
       " 'fucking',\n",
       " 'cluttered',\n",
       " 'reddit',\n",
       " 'male',\n",
       " 'emily',\n",
       " 'pathetic',\n",
       " 'resolve',\n",
       " 'ghost',\n",
       " 'quirky',\n",
       " 'earshot',\n",
       " 'dream',\n",
       " 'neet',\n",
       " 'vodka',\n",
       " 'cancel',\n",
       " 'instinct',\n",
       " 'teen',\n",
       " 'sadness',\n",
       " 'disorder',\n",
       " 'experiment',\n",
       " 'ebb',\n",
       " 'comprehend',\n",
       " 'everlaste',\n",
       " 'devastated',\n",
       " 'officially',\n",
       " 'wisdom',\n",
       " 'practically',\n",
       " 'tail',\n",
       " 'wow',\n",
       " 'visible',\n",
       " 'defend',\n",
       " 'convert',\n",
       " 'billion',\n",
       " 'sis',\n",
       " 'judgemental',\n",
       " 'hasove',\n",
       " 'loads',\n",
       " 'childporn',\n",
       " 'notice',\n",
       " 'manga',\n",
       " 'distort',\n",
       " 'letting',\n",
       " 'astigmatism',\n",
       " 'j',\n",
       " 'germany',\n",
       " 'groomed',\n",
       " 'dissopointment',\n",
       " 'chubby',\n",
       " 'rwholesomememe',\n",
       " 'tw',\n",
       " 'guidance',\n",
       " 'mske',\n",
       " 'body',\n",
       " 'crier',\n",
       " 'ruin',\n",
       " 'exs',\n",
       " 'ness',\n",
       " 'sunglass',\n",
       " 'maria',\n",
       " 'irritate',\n",
       " 'eventually',\n",
       " 'coincidence',\n",
       " 'frank',\n",
       " 'bcz',\n",
       " 'extraordinarily',\n",
       " 'attractiveness',\n",
       " 'unappealing',\n",
       " 'patting',\n",
       " 'magazine',\n",
       " 'bigger',\n",
       " 'coworker',\n",
       " 'tank',\n",
       " 'volounteer',\n",
       " 'somtime',\n",
       " 'fo',\n",
       " 'clap',\n",
       " 'significantly',\n",
       " 'mortify',\n",
       " 'qualify',\n",
       " 'sake',\n",
       " 'makeup',\n",
       " 'mebut',\n",
       " 'gather',\n",
       " 'km',\n",
       " 'time',\n",
       " 'illusion',\n",
       " 'aware',\n",
       " 'knowsit',\n",
       " 'psychatrist',\n",
       " 'evil',\n",
       " 'lovey',\n",
       " 'tipping',\n",
       " 'againyaddayadda',\n",
       " 'fcke',\n",
       " 'health',\n",
       " 'morenew',\n",
       " 'obsessiveness',\n",
       " 'shithead',\n",
       " 'esther',\n",
       " 'underestimatedand',\n",
       " 'gifted',\n",
       " 'specially',\n",
       " 'overweight',\n",
       " 'familyand',\n",
       " 'succumb',\n",
       " 'effect',\n",
       " 'tv',\n",
       " 'lesbian',\n",
       " 'destination',\n",
       " 'premature',\n",
       " 'fps',\n",
       " 'ifs',\n",
       " 'system',\n",
       " 'dm',\n",
       " 'assumption',\n",
       " 'haku',\n",
       " 'vase',\n",
       " 'fortunate',\n",
       " 'mundaneness',\n",
       " 'anecdote',\n",
       " 'daily',\n",
       " 'independently',\n",
       " 'trample',\n",
       " 'trip',\n",
       " 'bingo',\n",
       " 'nonsense',\n",
       " 'emotion',\n",
       " 'selfhelp',\n",
       " 'journaling',\n",
       " 'vise',\n",
       " 'adopt',\n",
       " 'packedstored',\n",
       " 'cook',\n",
       " 'misfit',\n",
       " 'flare',\n",
       " 'pushy',\n",
       " 'museum',\n",
       " 'accessible',\n",
       " 'shop',\n",
       " 'accomplishment',\n",
       " 'flatmate',\n",
       " 'expectancy',\n",
       " 'mote',\n",
       " 'bullshit',\n",
       " 'asexual',\n",
       " 'obvious',\n",
       " 'shared',\n",
       " 'brotherdad',\n",
       " 'batshittery',\n",
       " 'pasta',\n",
       " 'empathise',\n",
       " 'importance',\n",
       " 'angsty',\n",
       " 'drung',\n",
       " 'dedicate',\n",
       " 'gott',\n",
       " 'bipolar',\n",
       " 'rare',\n",
       " 'caresit',\n",
       " 'ought',\n",
       " 'masterpiece',\n",
       " 'shrink',\n",
       " 'freling',\n",
       " 'outthink',\n",
       " 'ferry',\n",
       " 'didint',\n",
       " 'retardation',\n",
       " 'ice',\n",
       " 'subculture',\n",
       " 'bounce',\n",
       " 'possession',\n",
       " 'fuck',\n",
       " 'vibin',\n",
       " 'sympathetic',\n",
       " 'moreso',\n",
       " 'horrified',\n",
       " 'anymorethis',\n",
       " 'inferior',\n",
       " 'esophagus',\n",
       " 'zero',\n",
       " 'tale',\n",
       " 'restrain',\n",
       " 'neurotically',\n",
       " 'conflicted',\n",
       " 'appraisal',\n",
       " 'chug',\n",
       " 'dreams',\n",
       " 'fastforward',\n",
       " 'thisnot',\n",
       " 'unfairly',\n",
       " 'mutilate',\n",
       " 'incurable',\n",
       " 'seperate',\n",
       " 'virtually',\n",
       " 'progress',\n",
       " 'waste',\n",
       " 'prone',\n",
       " 'itselffor',\n",
       " 'cantwont',\n",
       " 'talkative',\n",
       " 'complicated',\n",
       " 'plenty',\n",
       " 'beginner',\n",
       " 'everybody',\n",
       " 'dms',\n",
       " 'soothing',\n",
       " 'emptyif',\n",
       " 'pump',\n",
       " 'crushed',\n",
       " 'intel',\n",
       " 'pound',\n",
       " 'toand',\n",
       " 'minuscule',\n",
       " 'someday',\n",
       " 'look',\n",
       " 'wilt',\n",
       " 'onelike',\n",
       " 'lightyear',\n",
       " 'unhelpful',\n",
       " 'flood',\n",
       " 'texte',\n",
       " 'accuse',\n",
       " 'recycling',\n",
       " 'nervously',\n",
       " 'rptsd',\n",
       " 'frustrate',\n",
       " 'trivialise',\n",
       " 'idc',\n",
       " 'dissapear',\n",
       " 'meantione',\n",
       " 'inevitably',\n",
       " 'happyhopefully',\n",
       " 'wierd',\n",
       " 'courage',\n",
       " 'compose',\n",
       " 'county',\n",
       " 'area',\n",
       " 'sidehustle',\n",
       " 'undermine',\n",
       " 'duck',\n",
       " 'jest',\n",
       " 'truth',\n",
       " 'extra',\n",
       " 'intuition',\n",
       " 'nap',\n",
       " 'fs',\n",
       " 'psychologically',\n",
       " 'aged',\n",
       " 'sheeet',\n",
       " 'kindly',\n",
       " 'againshe',\n",
       " 'depersonalize',\n",
       " 'fireworks',\n",
       " 'left',\n",
       " 'potential',\n",
       " 'coincidentally',\n",
       " 'aircraft',\n",
       " 'encouragement',\n",
       " 'cosign',\n",
       " 'councillor',\n",
       " 'scarce',\n",
       " 'lane',\n",
       " 'strip',\n",
       " 'support',\n",
       " 'everyday',\n",
       " 'sible',\n",
       " 'pic',\n",
       " 'radio',\n",
       " 'brighten',\n",
       " 'swap',\n",
       " 'glover',\n",
       " 'pharmacist',\n",
       " 'arizona',\n",
       " 'softly',\n",
       " 'insinuate',\n",
       " 'cipralex',\n",
       " 'catholic',\n",
       " 'anytime',\n",
       " 'celexafree',\n",
       " 'bagging',\n",
       " 'quote',\n",
       " 'abroadare',\n",
       " 'comorbid',\n",
       " 'pisstake',\n",
       " 'irony',\n",
       " 'burn',\n",
       " 'pessimist',\n",
       " 'pour',\n",
       " 'usa',\n",
       " 'fantastic',\n",
       " 'reactionary',\n",
       " 'inclination',\n",
       " 'divorced',\n",
       " 'judgment',\n",
       " 'horrid',\n",
       " 'kobe',\n",
       " 'thin',\n",
       " 'christma',\n",
       " 'dreamed',\n",
       " 'format',\n",
       " 'lazystupid',\n",
       " 'depend',\n",
       " 'officer',\n",
       " 'meit',\n",
       " 'regardless',\n",
       " 'mock',\n",
       " 'listen',\n",
       " 'threshold',\n",
       " 'yeeting',\n",
       " 'map',\n",
       " 'uite',\n",
       " 'institutes',\n",
       " 'destruction',\n",
       " 'heartwarming',\n",
       " 'withered',\n",
       " 'asse',\n",
       " 'coffeeteabeverageofchoice',\n",
       " 'oil',\n",
       " 'comic_strip',\n",
       " 'ill',\n",
       " 'ar',\n",
       " 'wiper',\n",
       " 'girly',\n",
       " 'facility',\n",
       " 'chem',\n",
       " 'weigh',\n",
       " 'desperate',\n",
       " 'hospital',\n",
       " 'fasad',\n",
       " 'milestone',\n",
       " 'gamesdistracting',\n",
       " 'christ',\n",
       " 'brainer',\n",
       " 'criterion',\n",
       " 'therapy',\n",
       " 'woe',\n",
       " 'rdepression',\n",
       " 'entertainment',\n",
       " 'stabilise',\n",
       " 'skmethe',\n",
       " 'thursday',\n",
       " 'dasha',\n",
       " 'bfrb',\n",
       " 'painsuffere',\n",
       " 'transform',\n",
       " 'coke',\n",
       " 'bjs',\n",
       " 'ip',\n",
       " 'aback',\n",
       " 'worldi',\n",
       " 'positice',\n",
       " 'reconfirm',\n",
       " 'secound',\n",
       " 'peel',\n",
       " 'fusa',\n",
       " 'exclaimer',\n",
       " 'big',\n",
       " 'bigote',\n",
       " 'circlejerk',\n",
       " 'surprised',\n",
       " 'milkshake',\n",
       " 'onetime',\n",
       " 'enjoyed',\n",
       " 'youtuber',\n",
       " 'alzheimer',\n",
       " 'board',\n",
       " 'belly',\n",
       " 'intensionally',\n",
       " 'perceive',\n",
       " 'finna',\n",
       " 'throwing',\n",
       " 'substantial',\n",
       " 'fallen',\n",
       " 'infusion',\n",
       " 'short',\n",
       " 'awarding',\n",
       " 'quitter',\n",
       " 'european',\n",
       " 'startet',\n",
       " 'microdosing',\n",
       " 'underwater',\n",
       " 'new',\n",
       " 'sell',\n",
       " 'rant',\n",
       " 'sensation',\n",
       " 'closeby',\n",
       " 'soldier',\n",
       " 'alcoholism',\n",
       " 'abscense',\n",
       " 'freedom',\n",
       " 'tragic',\n",
       " 'existing',\n",
       " 'venlafaxine',\n",
       " 'infroknt',\n",
       " 'infinitely',\n",
       " 'aperantly',\n",
       " 'inactivity',\n",
       " 'simaltaniously',\n",
       " 'breakout',\n",
       " 'debilitate',\n",
       " 'reminiscing',\n",
       " 'phq',\n",
       " 'renovate',\n",
       " 'bet',\n",
       " 'run',\n",
       " 'resteraunt',\n",
       " 'contently',\n",
       " 'tread',\n",
       " 'season',\n",
       " 'football',\n",
       " 'sibling',\n",
       " 'problem',\n",
       " 'alphabet',\n",
       " 'oo',\n",
       " 'mornin',\n",
       " 'activation',\n",
       " 'dunkin',\n",
       " 'partway',\n",
       " 'toget',\n",
       " 'ablut',\n",
       " 'bleak',\n",
       " 'ju',\n",
       " 'v',\n",
       " 'sport',\n",
       " 'imprisonment',\n",
       " 'bug',\n",
       " 'strenght',\n",
       " 'towel',\n",
       " 'fry',\n",
       " 'cumme',\n",
       " 'aday',\n",
       " 'hormone',\n",
       " 'prosecution',\n",
       " 'u',\n",
       " 'foremost',\n",
       " 'understandable',\n",
       " 'alternative',\n",
       " 'exspecially',\n",
       " 'havend',\n",
       " 'issues',\n",
       " 'warning',\n",
       " 'ass',\n",
       " 'listener',\n",
       " 'coronovirus',\n",
       " 'attendance',\n",
       " 'cloudy',\n",
       " 'knock',\n",
       " 'devistate',\n",
       " 'kind',\n",
       " 'scrutinize',\n",
       " 'debt',\n",
       " 'downer',\n",
       " 'nihilistic',\n",
       " 'justreally',\n",
       " 'brake',\n",
       " 'kicking',\n",
       " 'haunt',\n",
       " 'unfit',\n",
       " 'lifenot',\n",
       " 'oof',\n",
       " 'exaggeration',\n",
       " 'flirty',\n",
       " 'drummer',\n",
       " 'narcissistic',\n",
       " 'fm',\n",
       " 'morty',\n",
       " 'tomorow',\n",
       " 'window',\n",
       " 'dramatic',\n",
       " 'jesse',\n",
       " 'consume',\n",
       " 'topic',\n",
       " 'differ',\n",
       " 'enforce',\n",
       " 'dreaded',\n",
       " 'lover',\n",
       " 'deficient',\n",
       " 'unbind',\n",
       " 'sigh',\n",
       " 'mast',\n",
       " 'gettibg',\n",
       " 'bird',\n",
       " 'awesome',\n",
       " 'hero',\n",
       " 'addict',\n",
       " 'misunderstanding',\n",
       " 'surname',\n",
       " 'head',\n",
       " 'positively',\n",
       " 'oneupsmanship',\n",
       " 'downhill',\n",
       " 'heshe',\n",
       " 'poster',\n",
       " 'rope',\n",
       " 'ridge',\n",
       " 'disintereste',\n",
       " 'speed',\n",
       " 'widow',\n",
       " 'visited',\n",
       " 'effective',\n",
       " 'presume',\n",
       " 'determination',\n",
       " 'denture',\n",
       " 'excistance',\n",
       " 'pleaser',\n",
       " 'fatigue',\n",
       " 'operation',\n",
       " 'dishwasher',\n",
       " 'inflict',\n",
       " 'wat',\n",
       " 'holding',\n",
       " 'dropping',\n",
       " 'boris',\n",
       " 'miiiight',\n",
       " 'squad',\n",
       " 'homecountry',\n",
       " 'infact',\n",
       " 'hookup',\n",
       " 'hadhe',\n",
       " 'selfie',\n",
       " 'yay',\n",
       " 'clarity',\n",
       " 'standing',\n",
       " 'connection',\n",
       " 'alcohol',\n",
       " 'knkw',\n",
       " 'cr',\n",
       " 'leap',\n",
       " 'homesick',\n",
       " 'taekwondo',\n",
       " 'continually',\n",
       " 'scrub',\n",
       " 'endure',\n",
       " 'suicidality',\n",
       " 'pi',\n",
       " 'nd',\n",
       " 'thirdly',\n",
       " 'bestow',\n",
       " 'didn',\n",
       " 'breakfast',\n",
       " 'clinging',\n",
       " 'unprompted',\n",
       " 'tragically',\n",
       " 'salon',\n",
       " 'deteriorate',\n",
       " 'impede',\n",
       " 'bam',\n",
       " 'nother',\n",
       " 'particularly',\n",
       " 'intensively',\n",
       " 'unhappiness',\n",
       " 'rap',\n",
       " 'blurt',\n",
       " 'beautiful',\n",
       " 'averagely',\n",
       " 'jumpstart',\n",
       " 'freeride',\n",
       " 'boarder',\n",
       " 'bushy',\n",
       " 'diagnosisim',\n",
       " 'flaky',\n",
       " 'bestfriend',\n",
       " 'culdesac',\n",
       " 'festival',\n",
       " 'murder',\n",
       " 'negate',\n",
       " 'uhhhhhhh',\n",
       " 'outburst',\n",
       " 'slug',\n",
       " 'ant',\n",
       " 'experince',\n",
       " 'spit',\n",
       " 'fiancé',\n",
       " 'bong',\n",
       " 'weeksmoth',\n",
       " 'everygirl',\n",
       " 'shadow',\n",
       " 'platform',\n",
       " 'avoidance',\n",
       " 'product',\n",
       " 'basic',\n",
       " 'oncoming',\n",
       " 'happyi',\n",
       " 'trivial',\n",
       " 'yesr',\n",
       " 'shelter',\n",
       " 'sabotage',\n",
       " 'allergy',\n",
       " 'blaming',\n",
       " 'parentsfriend',\n",
       " 'mindful',\n",
       " 'desparate',\n",
       " 'control',\n",
       " 'fortunately',\n",
       " 'realm',\n",
       " 'comfortbut',\n",
       " 'advertise',\n",
       " 'depressionanxiety',\n",
       " 'wait',\n",
       " 'understaffed',\n",
       " 'tyranny',\n",
       " 'corpse',\n",
       " 'horseman',\n",
       " 'freely',\n",
       " 'restore',\n",
       " 'bar',\n",
       " 'gossip',\n",
       " 'middleground',\n",
       " 'parentsi',\n",
       " 'unfixable',\n",
       " 'rid',\n",
       " 'pussy',\n",
       " 'hitcutting',\n",
       " 'therapyymed',\n",
       " 'naughty',\n",
       " 'thinkdo',\n",
       " 'crowded',\n",
       " 'rockier',\n",
       " 'honeslty',\n",
       " 'hurdle',\n",
       " 'downside',\n",
       " 'chickened',\n",
       " 'working',\n",
       " 'verbal',\n",
       " 'campaign',\n",
       " 'timethis',\n",
       " 'diagram',\n",
       " 'rushed',\n",
       " 'gaslighted',\n",
       " 'ceiling',\n",
       " 'meal',\n",
       " 'hunch',\n",
       " 'starbuck',\n",
       " 'running',\n",
       " 'formally',\n",
       " 'fuuuuuuuuuuuuuuuuck',\n",
       " 'ikea',\n",
       " 'essay',\n",
       " 'situation',\n",
       " 'perfecti',\n",
       " 'paycheck',\n",
       " 'quietisolation',\n",
       " 'autist',\n",
       " 'normality',\n",
       " 'rightnow',\n",
       " 'initially',\n",
       " 'deferred',\n",
       " 'person',\n",
       " 'collage',\n",
       " 'fascinate',\n",
       " 'amend',\n",
       " 'gate',\n",
       " 'iswa',\n",
       " 'normalplease',\n",
       " 'jeopardy',\n",
       " 'musicother',\n",
       " 'marine',\n",
       " 'admission',\n",
       " 'ease',\n",
       " 'section',\n",
       " 'lithium',\n",
       " 'solitaire',\n",
       " 'pig',\n",
       " 'semster',\n",
       " 'selfsabatoge',\n",
       " 'die',\n",
       " 'report',\n",
       " 'negatively',\n",
       " 'korea',\n",
       " 'paroxetine',\n",
       " 'peek',\n",
       " 'complicate',\n",
       " 'appreciation',\n",
       " 'meyou',\n",
       " 'knowif',\n",
       " 'responsible',\n",
       " 'broke',\n",
       " 'password',\n",
       " 'possible',\n",
       " 'partake',\n",
       " 'fk',\n",
       " 'hallsect',\n",
       " 'classic',\n",
       " 'fascination',\n",
       " 'lonelyit',\n",
       " 'indicate',\n",
       " 'massive',\n",
       " 'told',\n",
       " 'betterr',\n",
       " 'partially',\n",
       " 'insensible',\n",
       " 'impatient',\n",
       " 'porch',\n",
       " 'rub',\n",
       " 'prosperity',\n",
       " 'embarrassed',\n",
       " 'depreciation',\n",
       " 'promised',\n",
       " 'alreadu',\n",
       " 'drastic',\n",
       " 'smoked',\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(vocab1 + vocab2))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30331128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb4c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fecf27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train['Text_data'].to_numpy()\n",
    "word_freq = {}\n",
    "\n",
    "for text in texts:\n",
    "    for word in text.split():\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2542fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10272 unique tokens.\n",
      "Shape of data tensor: (8891, 40)\n",
      "Shape of label tensor: (8891,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "print('Found %s unique tokens.' % (num_words - 1))\n",
    "\n",
    "# pad \n",
    "data = pad_sequences(\n",
    "    sequences, \n",
    "    maxlen=MAX_SEQUENCE_LENGTH,\n",
    "    padding='post', \n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "labels = train['Label'].to_numpy()\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d075105",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad4b5552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['borderline', 'whoa', 'influence', 'flirt', 'remark', 'hollowed', 'hungry', 'unplanned', 'speaker', 'shift', 'kg', 'entwine', 'cali', 'layoff', 'pro', 'voice', 'morose', 'continously', 'construct', 'tongue', 'guilt', 'rewarding', 'routine', 'derealization', 'send', 'dissappeare', 'altercation', 'try', 'confession', 'scary', 'provoke', 'drift', 'craigslist', 'competitive', 'portrait', 'pay', 'ventrant', 'comedy', 'size', 'offender', 'rethink', 'fairytale', 'trust', 'longterm', 'deppresse', 'nocall', 'bing', 'babysit', 'underestimate', 'populated', 'healthcare', 'jjst', 'gig', 'holidays', 'exceed', 'immerse', 'dmv', 'admit', 'staff', 'london', 'thanks', 'lonleny', 'enter', 'certainty', 'lung', 'privilege', 'unfiltered', 'part', 'investigate', 'accept', 'heavy', 'complacency', 'mercy', 'likelyhood', 'panicing', 'ignorant', 'existto', 'pessimisticarguably', 'predispose', 'forsee', 'genuinely', 'statementand', 'kiddo', 'againwhatever', 'lovejust', 'talkspace', 'expect', 'we', 'breakdown', 'shocking', 'disappoint', 'hindrance', 'gesture', 'carry', 'watch', 'material', 'nag', 'exception', 'granpa', 'menot']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "x_train_vectorized = vectorizer.fit_transform(train['Text_data'])\n",
    "\n",
    "# print vocabulary\n",
    "print(vectorizer.get_feature_names()[2500:2600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9b2979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07b312a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train[\"Text_data\"]\n",
    "y = train[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddcf7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6075f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorization = TfidfVectorizer()\n",
    "xv_train = vectorization.fit_transform(x_train)\n",
    "xv_test = vectorization.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c957e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X_test, model_object):\n",
    "  \n",
    "    # Predicton on test with giniIndex\n",
    "    y_pred = model_object.predict(xv_test)\n",
    "    print(\"Predicted values:\")\n",
    "    print(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e4c9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(y_test, y_pred):\n",
    "      \n",
    "    print(\"Confusion Matrix: \",\n",
    "        confusion_matrix(y_test, y_pred))\n",
    "      \n",
    "    print (\"Accuracy : \",\n",
    "    accuracy_score(y_test,y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c39500ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "[1 1 1 ... 1 1 1]\n",
      "Confusion Matrix:  [[ 195  302    6]\n",
      " [  63 1425   14]\n",
      " [   8  140   70]]\n",
      "Accuracy :  76.0233918128655\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.39      0.51       503\n",
      "           1       0.76      0.95      0.85      1502\n",
      "           2       0.78      0.32      0.45       218\n",
      "\n",
      "    accuracy                           0.76      2223\n",
      "   macro avg       0.76      0.55      0.60      2223\n",
      "weighted avg       0.76      0.76      0.73      2223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision tree with gini\n",
    "model_gini = DecisionTreeClassifier(criterion = \"gini\",\n",
    "            random_state = 123,max_depth=10, min_samples_leaf=6)\n",
    "  \n",
    "# Performing training\n",
    "model_gini.fit(xv_train, y_train)\n",
    "\n",
    "# Prediction using gini\n",
    "y_pred_gini = prediction(xv_test, model_gini)\n",
    "cal_accuracy(y_test, y_pred_gini)\n",
    "print(classification_report(y_test,y_pred_gini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98fb973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "[0 1 1 ... 1 2 1]\n",
      "Confusion Matrix:  [[ 116  377   10]\n",
      " [   9 1482   11]\n",
      " [   5  101  112]]\n",
      "Accuracy :  76.92307692307693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.23      0.37       503\n",
      "           1       0.76      0.99      0.86      1502\n",
      "           2       0.84      0.51      0.64       218\n",
      "\n",
      "    accuracy                           0.77      2223\n",
      "   macro avg       0.83      0.58      0.62      2223\n",
      "weighted avg       0.80      0.77      0.72      2223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision tree with entropy\n",
    "model_entropy = DecisionTreeClassifier(\n",
    "            criterion = \"entropy\", random_state = 123,\n",
    "            max_depth = 10, min_samples_leaf = 6)\n",
    "  \n",
    "# Performing training\n",
    "model_entropy.fit(xv_train, y_train)\n",
    "\n",
    "# Prediction using entropy\n",
    "y_pred_entropy = prediction(xv_test, model_entropy)\n",
    "cal_accuracy(y_test, y_pred_entropy)\n",
    "\n",
    "print(classification_report(y_test,y_pred_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc16bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31a70a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e581e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xv_train = vectorizer.fit_transform(x_train)\n",
    "xv_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b42694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "[0 1 1 ... 1 1 1]\n",
      "Confusion Matrix:  [[ 167  333    3]\n",
      " [  53 1446    3]\n",
      " [  12  144   62]]\n",
      "Accuracy :  75.34862798020693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.33      0.45       503\n",
      "           1       0.75      0.96      0.84      1502\n",
      "           2       0.91      0.28      0.43       218\n",
      "\n",
      "    accuracy                           0.75      2223\n",
      "   macro avg       0.79      0.53      0.58      2223\n",
      "weighted avg       0.76      0.75      0.72      2223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision tree with gini\n",
    "model_gini = DecisionTreeClassifier(criterion = \"gini\",\n",
    "            random_state = 123,max_depth=10, min_samples_leaf=6)\n",
    "  \n",
    "# Performing training\n",
    "model_gini.fit(xv_train, y_train)\n",
    "\n",
    "# Prediction using gini\n",
    "y_pred_gini = prediction(xv_test, model_gini)\n",
    "cal_accuracy(y_test, y_pred_gini)\n",
    "print(classification_report(y_test,y_pred_gini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cfe03d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "[0 1 1 ... 1 1 1]\n",
      "Confusion Matrix:  [[ 113  384    6]\n",
      " [   3 1490    9]\n",
      " [   1  110  107]]\n",
      "Accuracy :  76.92307692307693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.22      0.36       503\n",
      "           1       0.75      0.99      0.85      1502\n",
      "           2       0.88      0.49      0.63       218\n",
      "\n",
      "    accuracy                           0.77      2223\n",
      "   macro avg       0.86      0.57      0.62      2223\n",
      "weighted avg       0.81      0.77      0.72      2223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision tree with entropy\n",
    "model_entropy = DecisionTreeClassifier(\n",
    "            criterion = \"entropy\", random_state = 123,\n",
    "            max_depth = 10, min_samples_leaf = 6)\n",
    "  \n",
    "# Performing training\n",
    "model_entropy.fit(xv_train, y_train)\n",
    "\n",
    "# Prediction using entropy\n",
    "y_pred_entropy = prediction(xv_test, model_entropy)\n",
    "cal_accuracy(y_test, y_pred_entropy)\n",
    "\n",
    "print(classification_report(y_test,y_pred_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "caa4c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0008acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# model = gensim.models.Word2Vec() \n",
    "# GoogleModel = gensim.models.KeyedVectors.load_word2vec_format(r'C:\\Users\\SHWETANK VERMA\\Documents\\Mlstuff\\Major-1\\GoogleNews-vectors-negative300.bin', binary=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fdb7aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "import gensim\n",
    "X_train, X_test, y_train, y_test = train_test_split (train['Text_data'], train['Label'] , test_size=0.2)\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size=100,\n",
    "                                   window=5,\n",
    "                                   min_count=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0d4c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(w2v_model.wv.index_to_key )\n",
    "X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_train])\n",
    "X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b1bdb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentence vectors by averaging the word vectors for the words contained in the sentence\n",
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(100, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36784228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, min_samples_leaf=6, random_state=123)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision tree with gini\n",
    "model_gini = DecisionTreeClassifier(criterion = \"gini\",\n",
    "            random_state = 123,max_depth=10, min_samples_leaf=6)\n",
    "  \n",
    "# Performing training\n",
    "model_gini.fit(X_train_vect_avg, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1c944b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1779"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_vect_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58164895",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 12150 features, but DecisionTreeClassifier is expecting 100 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6992\\1932407939.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Prediction using gini\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_vect_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_gini\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcal_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6992\\1740854886.py\u001b[0m in \u001b[0;36mprediction\u001b[1;34m(X_test, model_object)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Predicton on test with giniIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxv_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Predicted values:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\latest\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m    466\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\latest\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;34m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             if issparse(X) and (\n\u001b[0;32m    435\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\latest\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\latest\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    401\u001b[0m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[1;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X has 12150 features, but DecisionTreeClassifier is expecting 100 features as input."
     ]
    }
   ],
   "source": [
    "\n",
    "# Prediction using gini\n",
    "y_pred = prediction(X_test_vect_avg, model_gini)\n",
    "cal_accuracy(y_test,y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279c810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
